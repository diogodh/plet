{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GMSCcHh1LScM"},"outputs":[],"source":["!pip install --upgrade pip\n","!pip install -q mediapipe==0.10.0\n","\n","#tested on:\n","# Python 3.10.12\n","# pip 23.3.2\n","# mediapipe-model-maker 0.2.1.3 ???"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oazmbPzKHYFq"},"outputs":[],"source":["from google.colab import files\n","import os\n","import json\n","import tensorflow as tf\n","assert tf.__version__.startswith('2')\n","\n","from mediapipe_model_maker import object_detector"]},{"cell_type":"markdown","source":["**TEST**"],"metadata":{"id":"acVlX1oMMApG"}},{"cell_type":"code","source":["#@markdown We implemented some functions to visualize the object detection results. <br/> Run the following cell to activate the functions.\n","import cv2\n","import numpy as np\n","\n","MARGIN = 10  # pixels\n","ROW_SIZE = 10  # pixels\n","FONT_SIZE = 1\n","FONT_THICKNESS = 1\n","TEXT_COLOR = (255, 0, 0)  # red\n","\n","\n","def visualize(\n","    image,\n","    detection_result\n",") -> np.ndarray:\n","  \"\"\"Draws bounding boxes on the input image and return it.\n","  Args:\n","    image: The input RGB image.\n","    detection_result: The list of all \"Detection\" entities to be visualize.\n","  Returns:\n","    Image with bounding boxes.\n","  \"\"\"\n","  for detection in detection_result.detections:\n","    # Draw bounding_box\n","    bbox = detection.bounding_box\n","    start_point = bbox.origin_x, bbox.origin_y\n","    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n","    cv2.rectangle(image, start_point, end_point, TEXT_COLOR, 3)\n","\n","    # Draw label and score\n","    category = detection.categories[0]\n","    category_name = category.category_name\n","    probability = round(category.score, 2)\n","    result_text = category_name + ' (' + str(probability) + ')'\n","    text_location = (MARGIN + bbox.origin_x,\n","                     MARGIN + ROW_SIZE + bbox.origin_y)\n","    cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n","                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n","\n","  return image"],"metadata":{"id":"bwCPyxPsPLar"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget 'https://raw.githubusercontent.com/diogodh/plet/master/test_img.jpg'\n","!wget 'https://raw.githubusercontent.com/diogodh/plet/master/test_img2.jpg'\n","!wget 'https://raw.githubusercontent.com/diogodh/plet/master/exported_model/model.tflite'\n","\n","IMAGE_FILE = '/content/test_img.jpg'\n","IMAGE_FILE2 = '/content/test_img2.jpg'\n","\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","img = cv2.imread(IMAGE_FILE)\n","cv2_imshow(img)\n","img2 = cv2.imread(IMAGE_FILE2)\n","cv2_imshow(img2)"],"metadata":{"id":"bsA0t5cOMIVP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STEP 1: Import the necessary modules.\n","import numpy as np\n","import mediapipe as mp\n","from mediapipe.tasks import python\n","from mediapipe.tasks.python import vision\n","\n","# STEP 2: Create an ObjectDetector object.\n","base_options = python.BaseOptions(model_asset_path='/content/model.tflite')\n","\n","options = vision.ObjectDetectorOptions(base_options=base_options,\n","                                       score_threshold=0.2)\n","\n","detector = vision.ObjectDetector.create_from_options(options)\n","\n","# STEP 3: Load the input image.\n","image = mp.Image.create_from_file(IMAGE_FILE)\n","\n","# STEP 4: Detect objects in the input image.\n","detection_result = detector.detect(image)\n","\n","# STEP 5: Process the detection result. In this case, visualize it.\n","image_copy = np.copy(image.numpy_view())\n","annotated_image = visualize(image_copy, detection_result)\n","rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n","cv2_imshow(rgb_annotated_image)"],"metadata":{"id":"zUqYCXx8NQXE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"npaRBUB3ZevY"},"source":["## Hyperparameters\n","You can further customize the model using the ObjectDetectorOptions class, which has three parameters for `SupportedModels`, `ModelOptions`, and `HParams`.\n","\n","Use the `SupportedModels` enum class to specify the model architecture to use for training. The following model architectures are supported:\n","* MOBILENET_V2\n","* MOBILENET_V2_I320\n","* MOBILENET_MULTI_AVG\n","* MOBILENET_MULTI_AVG_I384\n","\n","Use the `HParams` class to customize other parameters related to training and saving the model:\n","* `learning_rate`: Learning rate to use for gradient descent training. Defaults to 0.3.\n","* `batch_size`: Batch size for training. Defaults to 8.\n","* `epochs`: Number of training iterations over the dataset. Defaults to 30.\n","* `cosine_decay_epochs`: The number of epochs for cosine decay learning rate. See [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay) for more info. Defaults to None, which is equivalent to setting it to `epochs`.\n","* `cosine_decay_alpha`: The alpha value for cosine decay learning rate. See [tf.keras.optimizers.schedules.CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay) for more info. Defaults to 1.0, which means no cosine decay.\n","\n","Use the `ModelOptions` class to customize parameters related to the model itself:\n","* `l2_weight_decay`: L2 regularization penalty used in [tf.keras.regularizers.L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2). Defaults to 3e-5.\n","\n","Uset the `QATHParams` class to customize training parameters for Quantization Aware Training:\n","* `learning_rate`: Learning rate to use for gradient descent QAT. Defaults to 0.3.\n","* `batch_size`: Batch size for QAT. Defaults to 8\n","* `epochs`: Number of training iterations over the dataset. Defaults to 15.\n","* `decay_steps`: Learning rate decay steps for Exponential Decay. See [tf.keras.optimizers.schedules.ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) for more information. Defaults to 8\n","* `decay_rate`: Learning rate decay rate for Exponential Decay. See [tf.keras.optimizers.schedules.ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) for more information. Defaults to 0.96."]},{"cell_type":"markdown","metadata":{"id":"9HCrUl8z6liX"},"source":["## Benchmarking\n","Below is a summary of our benchmarking results for the supported model architectures. These models were trained and evaluated on the same android figurines dataset as this notebook. When considering the model benchmarking results, there are a few important caveats to keep in mind:\n","* The android figurines dataset is a small and simple dataset with 62 training examples and 10 validation examples. Since the dataset is quite small, metrics may vary drastically due to variances in the training process. This dataset was provided for demo purposes and it is recommended to collect more data samples for better performing models.\n","* The float32 models were trained with the default HParams, and the QAT step for the int8 models was run with `QATHParams(learning_rate=0.1, batch_size=4, epochs=30, decay_rate=1)`.\n","* For your own dataset, you will likely need to tune values for both HParams and QATHParams in order to achieve the best results. See the [Hyperparameters](#hyperparameters) section above for more information on configuring training parameters.\n","* All latency numbers are benchmarked on the Pixel 6.\n","\n","\n","<table>\n","<thead>\n","<col>\n","<col>\n","<colgroup span=\"2\"></colgroup>\n","<colgroup span=\"2\"></colgroup>\n","<colgroup span=\"2\"></colgroup>\n","<tr>\n","<th rowspan=\"2\">Model architecture</th>\n","<th rowspan=\"2\">Input Image Size</th>\n","<th colspan=\"2\" scope=\"colgroup\">Test AP</th>\n","<th colspan=\"2\" scope=\"colgroup\">CPU Latency</th>\n","<th colspan=\"2\" scope=\"colgroup\">Model Size</th>\n","</tr>\n","<tr>\n","<th>float32</th>\n","<th>QAT int8</th>\n","<th>float32</th>\n","<th>QAT int8</th>\n","<th>float32</th>\n","<th>QAT int8</th>\n","</tr>\n","</thead>\n","<tbody>\n","<tr>\n","<td>MobileNetV2</td>\n","<td>256x256</td>\n","<td>88.4%</td>\n","<td>73.5%</td>\n","<td>48ms</td>\n","<td>16ms</td>\n","<td>11MB</td>\n","<td>3.2MB</td>\n","</tr>\n","<tr>\n","<td>MobileNetV2 I320</td>\n","<td>320x320</td>\n","<td>89.1%</td>\n","<td>75.5%</td>\n","<td>75ms</td>\n","<td>33.38ms</td>\n","<td>10MB</td>\n","<td>3.3MB</td>\n","</tr>\n","<tr>\n","<td>MobileNet MultiHW AVG</td>\n","<td>256x256</td>\n","<td>88.5%</td>\n","<td>70.0%</td>\n","<td>56ms</td>\n","<td>19ms</td>\n","<td>13MB</td>\n","<td>3.6MB</td>\n","</tr>\n","<tr>\n","<td>MobileNet MultiHW AVG I384</td>\n","<td>384x384</td>\n","<td>92.7%</td>\n","<td>73.4%</td>\n","<td>238ms</td>\n","<td>41ms</td>\n","<td>13MB</td>\n","<td>3.6MB</td>\n","</tr>\n","\n","</tbody>\n","</table>\n","\n"]}],"metadata":{"colab":{"last_runtime":{"build_target":"//learning/grp/tools/ml_python:ml_notebook","kind":"private"},"private_outputs":true,"provenance":[{"file_id":"1MBmwKhbpsVjhe9x1mblSiWLWFQ3hXMsJ","timestamp":1703764556556},{"file_id":"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/object_detector.ipynb","timestamp":1703593896907},{"file_id":"11PG1YgsQWWLJ8jpqJ6QY7hjYWzxVwoCb","timestamp":1677706798050}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}